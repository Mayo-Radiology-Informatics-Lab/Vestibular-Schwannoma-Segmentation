{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNetR Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a complete pipeline for performing 3D medical image segmentation using the UNETR architecture. \n",
    "\n",
    "It has been prepared based on the following original study:\n",
    "\n",
    "[1]: Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H.R. and Xu, D., 2022. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 574-584).\n",
    "\n",
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q \"monai-weekly[nibabel, tqdm, einops]\"\n",
    "#!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# MONAI modules\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandAdjustContrastd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ResizeWithPadOrCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    RandRotate90d,\n",
    ")\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "# SimpleITK for medical image processing\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_value = 12345\n",
    "random.seed(seed_value)               \n",
    "np.random.seed(seed_value)            \n",
    "torch.manual_seed(seed_value)   \n",
    "torch.cuda.manual_seed(seed_value) \n",
    "torch.cuda.manual_seed_all(seed_value) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.Specify Dataset Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders for directory paths\n",
    "data_dir = \"<DATA_DIR>\"                          # Path to the dataset directory, e.g., '/path/to/your/dataset'\n",
    "results_base_dir = \"<RESULTS_BASE_DIR>\"          # Path to the base directory for saving results, e.g., '/path/to/your/results'\n",
    "\n",
    "# Ensure the results directory exists; if not, create it\n",
    "os.makedirs(results_base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_windowing_range(image_dir, lower_percentile=5, upper_percentile=95, sample_size=5000, exclude_zeros=True):\n",
    "    \"\"\"\n",
    "    Calculates a windowing range for image intensity values by analyzing specified percentiles\n",
    "    (e.g., 5th and 95th percentiles). It avoids outliers by sampling random pixel values from images.\n",
    "    \n",
    "    Args:\n",
    "    - image_dir (str): Directory containing NIfTI (.nii.gz) format images.\n",
    "    - lower_percentile (int, optional): Lower percentile threshold. Default is 5.\n",
    "    - upper_percentile (int, optional): Upper percentile threshold. Default is 95.\n",
    "    - sample_size (int, optional): Number of random pixel values to sample from each image. Default is 5000.\n",
    "    - exclude_zeros (bool, optional): Whether to exclude zero-intensity (background/air) pixels. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    - lower_bound (float): Lower bound of the windowing range.\n",
    "    - upper_bound (float): Upper bound of the windowing range.\n",
    "    \"\"\"\n",
    "    all_sampled_values = []  # Store intensity values sampled from all images\n",
    "\n",
    "    # Filter for .nii.gz files only\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.nii.gz')]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = sitk.ReadImage(image_path)\n",
    "        image_np = sitk.GetArrayFromImage(image)\n",
    "        \n",
    "        # Exclude zero-intensity pixels (optional)\n",
    "        if exclude_zeros:\n",
    "            image_np = image_np[image_np != 0]\n",
    "        \n",
    "        # Randomly sample pixel values from the image (to reduce memory usage for large images)\n",
    "        if image_np.size > 0:  # Ensure there are non-zero pixels\n",
    "            sampled_values = np.random.choice(image_np.flatten(), size=min(sample_size, image_np.size), replace=False)\n",
    "            all_sampled_values.extend(sampled_values)  # Combine sampled values\n",
    "\n",
    "    # Compute percentiles from all sampled values\n",
    "    all_sampled_values = np.array(all_sampled_values)\n",
    "    \n",
    "    # Visualize the intensity distribution (optional)\n",
    "    plt.hist(all_sampled_values, bins=100)\n",
    "    plt.title('Intensity Distribution')\n",
    "    plt.xlabel('Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate percentiles\n",
    "    lower_bound = np.percentile(all_sampled_values, lower_percentile)\n",
    "    upper_bound = np.percentile(all_sampled_values, upper_percentile)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Specify the path to the image dataset\n",
    "image_dir = os.path.join(data_dir, 'imagesTr')\n",
    "\n",
    "# Calculate windowing range between 5th and 95th percentiles, excluding zero values\n",
    "lower_bound, upper_bound = calculate_windowing_range(image_dir)\n",
    "\n",
    "print(f\"Recommended windowing range: [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=16, #0, #-175,\n",
    "            a_max=1668, #1128, #250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        # Resize images to ensure dimensions are divisible by 16\n",
    "        ResizeWithPadOrCropd(keys=[\"image\", \"label\"], spatial_size=(224, 224, 64)),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(224, 224, 64),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "            allow_smaller=True  # Allow cropping even if image is smaller than spatial_size\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=(-0.1, 0.1),\n",
    "            prob=0.30,\n",
    "        ),\n",
    "        RandAdjustContrastd(\n",
    "            keys=[\"image\"],\n",
    "            prob=0.30,\n",
    "            gamma=(0.8, 1.2)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=lower_bound,\n",
    "            a_max=upper_bound,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        # Ensure validation images also match the expected size\n",
    "        ResizeWithPadOrCropd(keys=[\"image\", \"label\"], spatial_size=(224, 224, 64)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Data Loading and Preparation\n",
    "\n",
    "1. Download dataset\n",
    "2. Put images in the ./data/imagesTr\n",
    "3. Put labels in the ./data/labelsTr\n",
    "4. make JSON file accordingly: ./data/dataset_0.json\n",
    "\n",
    "Example of JSON file:\n",
    "\n",
    "        {\n",
    "    \"description\": \"btcv yucheng\",\n",
    "    \"labels\": {\n",
    "        \"0\": \"background\",\n",
    "        \"1\": \"spleen\",\n",
    "        \"2\": \"rkid\",\n",
    "        \"3\": \"lkid\",\n",
    "        \"4\": \"gall\",\n",
    "        \"5\": \"eso\",\n",
    "        \"6\": \"liver\",\n",
    "        \"7\": \"sto\",\n",
    "        \"8\": \"aorta\",\n",
    "        \"9\": \"IVC\",\n",
    "        \"10\": \"veins\",\n",
    "        \"11\": \"pancreas\",\n",
    "        \"12\": \"rad\",\n",
    "        \"13\": \"lad\"\n",
    "    },\n",
    "    \"licence\": \"yt\",\n",
    "    \"modality\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"name\": \"btcv\",\n",
    "    \"numTest\": 20,\n",
    "    \"numTraining\": 80,\n",
    "    \"reference\": \"Vanderbilt University\",\n",
    "    \"release\": \"1.0 06/08/2015\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"test\": [\n",
    "        \"imagesTs/img0061.nii.gz\",\n",
    "        \"imagesTs/img0062.nii.gz\",\n",
    "        ...\n",
    "        \"imagesTs/img0080.nii.gz\"\n",
    "    ],\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0001.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0001.nii.gz\"\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0034.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0034.nii.gz\"\n",
    "        }\n",
    "    ],\n",
    "    \"validation\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0035.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0035.nii.gz\"\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "            \"image\": \"imagesTr/img0040.nii.gz\",\n",
    "            \"label\": \"labelsTr/label0040.nii.gz\"\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"/path/yo/your/json/file\"\n",
    "\n",
    "datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "train_ds = CacheDataset(\n",
    "    data=datalist,\n",
    "    transform=train_transforms,\n",
    "    cache_num=24,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=8,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=6, cache_rate=1.0, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_slices(data_loader):\n",
    "    \"\"\"\n",
    "    Visualizes 5 slices around the center slice from a batch of images and labels.\n",
    "\n",
    "    Args:\n",
    "    - data_loader (DataLoader): A PyTorch DataLoader providing 3D medical image data.\n",
    "\n",
    "    Returns:\n",
    "    None. Displays the visualization using matplotlib.\n",
    "    \"\"\"\n",
    "    # Get a batch of data\n",
    "    batch_data = next(iter(data_loader))\n",
    "    images = batch_data['image']\n",
    "    labels = batch_data['label']\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    images_np = images[0].numpy()\n",
    "    labels_np = labels[0].numpy()\n",
    "\n",
    "    # Center slice index along the z-axis\n",
    "    center_slice_idx = images_np.shape[-1] // 2\n",
    "    slice_indices = [center_slice_idx - 24, center_slice_idx - 12, center_slice_idx, center_slice_idx + 12, center_slice_idx + 24]\n",
    "    \n",
    "    # Plot image slices, label slices, and overlays\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        axes[0, i].imshow(images_np[0, :, :, slice_idx], cmap='gray')\n",
    "        axes[0, i].set_title(f\"Image Slice {slice_idx}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(labels_np[0, :, :, slice_idx], cmap='gray')\n",
    "        axes[1, i].set_title(f\"Label Slice {slice_idx}\")\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        axes[2, i].imshow(images_np[0, :, :, slice_idx], cmap='gray')\n",
    "        axes[2, i].imshow(labels_np[0, :, :, slice_idx], cmap='Greens', alpha=0.5)\n",
    "        axes[2, i].set_title(f\"Overlay Slice {slice_idx}\")\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_slices(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Pipeline\n",
    "\n",
    "### 4.1. Dynamic Training Session Management\n",
    "This section dynamically determines the next training session number based on existing folders in the results directory. It creates a new directory for the current training session, including a timestamp and dataset name, ensuring all results are organized systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the next training number dynamically\n",
    "def get_next_training_number(results_base_dir):\n",
    "    # Ensure the results directory exists\n",
    "    if not os.path.exists(results_base_dir):\n",
    "        os.makedirs(results_base_dir)\n",
    "\n",
    "    # Get all subdirectories that start with 'train'\n",
    "    existing_dirs = [d for d in os.listdir(results_base_dir) if os.path.isdir(os.path.join(results_base_dir, d)) and re.match(r'train\\d{3}', d)]\n",
    "\n",
    "    # If there are no existing directories, start with train001\n",
    "    if not existing_dirs:\n",
    "        return 'train001'\n",
    "\n",
    "    # Extract the numbers from the folder names (e.g., 'train001', 'train002', ...)\n",
    "    existing_numbers = [int(re.search(r'train(\\d{3})', d).group(1)) for d in existing_dirs]\n",
    "    \n",
    "    # Determine the next number\n",
    "    next_number = max(existing_numbers) + 1\n",
    "    return f'train{next_number:03d}'  # Ensure it's zero-padded to 3 digits\n",
    "\n",
    "# Create a folder for the current training session\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "training_number = get_next_training_number(results_base_dir)\n",
    "train_name = 'schwannoma'  # You can update this to reflect the dataset you're using\n",
    "results_save_dir = os.path.join(results_base_dir, f\"{training_number}_{train_name}_{timestamp}\")\n",
    "os.makedirs(results_save_dir, exist_ok=True)\n",
    "\n",
    "config_file_path = os.path.join(results_save_dir, 'configs.txt')\n",
    "with open(config_file_path, 'w') as file:\n",
    "    pass \n",
    "\n",
    "print(results_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.GPU Selection and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "def get_free_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU available, using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    # Get GPU memory information using nvidia-smi\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=True\n",
    "        )\n",
    "        free_memory = [int(x) for x in result.stdout.decode('utf-8').strip().split('\\n')]\n",
    "        free_gpus = [(i, mem) for i, mem in enumerate(free_memory)]\n",
    "        \n",
    "        # Select the GPU with the most free memory\n",
    "        best_gpu = max(free_gpus, key=lambda x: x[1])[0]\n",
    "        print(f\"Using GPU: {best_gpu} with {free_memory[best_gpu]} MB free memory.\")\n",
    "        return torch.device(f\"cuda:{best_gpu}\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error using nvidia-smi:\", e)\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Automatically select the GPU with the most available memory\n",
    "device = get_free_gpu()\n",
    "print(device)\n",
    "# GPU ID'sini ECC hatasına göre manuel belirleyin\n",
    "device = torch.device(\"cuda:0\")  # ECC hatasız GPU\n",
    "\n",
    "\n",
    "model = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    img_size=(224, 224, 64),\n",
    "    feature_size=32,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    proj_type=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True, lambda_dice=1.5, lambda_ce=0.5)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Training Loop with Early Stopping\n",
    "\n",
    "This section defines the training loop for the model. It includes:\n",
    "\n",
    "1. Early Stopping: Stops training if validation performance does not improve for a defined number of epochs (patience).\n",
    "2. Loss and Metric Visualization: Plots and saves training loss and validation Dice metrics after each epoch.\n",
    "3. Best Model Saving: Automatically saves the best-performing model based on validation Dice scores.\n",
    "4. Time Limit: The training process is constrained to a specified time limit (e.g., 5 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patience for early stopping: Stops training if no improvement after 'patience' epochs\n",
    "patience = 10\n",
    "no_improvement_count = 0  # Counter for epochs without improvement\n",
    "min_delta = 0.001  # Minimum improvement difference to be considered significant\n",
    "\n",
    "# Function to plot and save loss and dice values\n",
    "def plot_and_save_loss_dice(epoch_loss_values, metric_values, global_step, epoch_num, elapsed_time, results_save_dir):\n",
    "    # Check if epoch_loss_values and metric_values are empty or constant\n",
    "    if not epoch_loss_values or not metric_values:\n",
    "        print(\"Epoch loss values or metric values are empty!\")\n",
    "    else:\n",
    "        print(f\"Epoch loss values: {epoch_loss_values}\")\n",
    "        print(f\"Metric values: {metric_values}\")\n",
    "        \n",
    "    days = int(elapsed_time // (24 * 3600))\n",
    "    hours = int((elapsed_time % (24 * 3600)) // 3600)\n",
    "    minutes = int((elapsed_time % 3600) // 60)\n",
    "\n",
    "    plt.figure(\"train\", (12, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Iteration Average Loss (Epoch {epoch_num} - {days}d {hours}:{minutes})\")\n",
    "    x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "    y = epoch_loss_values\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    # Plot Dice\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f\"Val Mean Dice (Epoch {epoch_num} - {days}days {hours}:{minutes})\")\n",
    "    x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "    y = metric_values\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to the specified path\n",
    "    loss_path = os.path.join(results_save_dir, f\"loss.png\")\n",
    "    plt.savefig(loss_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {loss_path}\")\n",
    "\n",
    "# Training function\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best, no_improvement_count):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Training ({global_step} Steps) (loss=X.X)\",\n",
    "        dynamic_ncols=True,\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        x, y = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
    "        logit_map = model(x)\n",
    "        loss = loss_function(logit_map, y)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 10 == 0:\n",
    "            epoch_iterator.set_description(\n",
    "                f\"Training ({global_step} Steps) (loss={loss.item():.5f})\"\n",
    "            )\n",
    "        if global_step % eval_num == 0 and global_step != 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_iterator_val = tqdm(\n",
    "                    val_loader,\n",
    "                    desc=\"Validation\",\n",
    "                    dynamic_ncols=True,\n",
    "                    position=1,\n",
    "                    leave=False\n",
    "                )\n",
    "                dice_metric.reset()\n",
    "                for val_step, batch in enumerate(epoch_iterator_val):\n",
    "                    val_inputs, val_labels = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
    "                    val_outputs = sliding_window_inference(val_inputs, (224, 224, 64), 4, model)\n",
    "                    val_labels_list = decollate_batch(val_labels)\n",
    "                    val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "                    val_outputs_list = decollate_batch(val_outputs)\n",
    "                    val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                    dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "                dice_val = dice_metric.aggregate().item()\n",
    "                dice_metric.reset()\n",
    "            model.train()\n",
    "            epoch_loss /= (step + 1)\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            metric_values.append(dice_val)\n",
    "\n",
    "            # Early Stopping check\n",
    "            if dice_val > dice_val_best + min_delta:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                no_improvement_count = 0  # Reset counter\n",
    "                best_model_path = os.path.join(results_save_dir, f\"best_model.pth\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                tqdm.write(\n",
    "                    f\"Model Was Saved! Current Best Avg. Dice: {dice_val_best:.4f} Current Avg. Dice: {dice_val:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                no_improvement_count += 1  # Increment the counter\n",
    "                tqdm.write(\n",
    "                    f\"Model Was Not Saved. Current Best Avg. Dice: {dice_val_best:.4f} Current Avg. Dice: {dice_val:.4f}\"\n",
    "                )\n",
    "\n",
    "            last_model_path = os.path.join(results_save_dir, f\"last_model.pth\")\n",
    "            torch.save(model.state_dict(), last_model_path)\n",
    "            \n",
    "            # Early stopping if no improvement for 'patience' epochs\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f\"Early stopping triggered after {no_improvement_count} epochs with no improvement.\")\n",
    "                break\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # Plot and save after each epoch\n",
    "    elapsed_time = time.time() - start_time\n",
    "    plot_and_save_loss_dice(epoch_loss_values, metric_values, global_step, epoch_num, elapsed_time, results_save_dir)\n",
    "    \n",
    "    return global_step, dice_val_best, global_step_best, no_improvement_count\n",
    "\n",
    "# Define time limit: 5 days in seconds\n",
    "time_limit = 6 * 60 #5 * 24 * 60 * 60\n",
    "eval_num = 500\n",
    "post_label = AsDiscrete(to_onehot=2)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=2)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "epoch_num = 1\n",
    "# Start training loop (without max_iterations)\n",
    "while True:\n",
    "    # Check if the elapsed time has exceeded the time limit\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > time_limit:\n",
    "        print(f\"Stopping training after {elapsed_time / 3600:.2f} hours as the time limit of 5 days has been reached.\")\n",
    "        break\n",
    "\n",
    "    # Continue training\n",
    "    global_step, dice_val_best, global_step_best, no_improvement_count = train(\n",
    "        global_step, train_loader, dice_val_best, global_step_best, no_improvement_count\n",
    "    )\n",
    "    epoch_num += 1\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training completed in {elapsed_time / 3600:.2f} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(os.path.join(results_save_dir, \"best_metric_model.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Finalizing Training and Organizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")\n",
    "\n",
    "# Check if the training was completed without errors and rename the folder\n",
    "completed_folder_name = results_save_dir + \"_completed\"\n",
    "os.rename(results_save_dir, completed_folder_name)\n",
    "\n",
    "print(f\"Training folder renamed to: {completed_folder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Save the plot to the specified path\n",
    "loss_path = os.path.join(results_save_dir + '_completed', f\"loss_final.png\")\n",
    "plt.savefig(loss_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Plot saved to {loss_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unetr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
